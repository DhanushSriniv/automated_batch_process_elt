"""
Live Data Endpoint Comparison

Purpose:
    Fetch and compare actual live data from both the bike-share-json and 
    gbfs-specification endpoints to see if the actual data is identical.
    
This helps determine:
    1. If the DATA from both sources is the same
    2. Performance differences between endpoints
    3. Data quality and completeness
"""

import os
import sys
import json
import time
from datetime import datetime
from typing import Dict, Tuple

# Dynamically find the project root
current_dir = os.path.dirname(os.path.abspath(__file__))
while not os.path.exists(os.path.join(current_dir, 'utils')):
    parent = os.path.dirname(current_dir)
    if parent == current_dir:
        raise RuntimeError("Could not find project root (utils folder not found)")
    current_dir = parent
sys.path.insert(0, current_dir)

from utils.el_global import (
    load_json_from_file,
    fetch_json,
    compare_json
)


def load_endpoints_from_metadata():
    """Load endpoint URLs from the metadata files."""
    bike_share_path = os.path.join(current_dir, "data/gbfs_feeds/feeds_data/bike-share-json/feeds_summary.json")
    
    metadata = load_json_from_file(bike_share_path)
    endpoints = {}
    
    for feed_name, feed_info in metadata.get("feeds", {}).items():
        endpoints[feed_name] = feed_info.get("url")
    
    return endpoints


def fetch_with_timing(url: str) -> Tuple[dict, float]:
    """Fetch data from URL and measure response time."""
    start_time = time.time()
    try:
        data = fetch_json(url)
        elapsed_time = time.time() - start_time
        return data, elapsed_time
    except Exception as e:
        elapsed_time = time.time() - start_time
        print(f"  ‚ùå Error fetching {url}: {str(e)}")
        return None, elapsed_time


def compare_endpoint_data(endpoint_name: str, url: str):
    """Fetch and compare data from an endpoint."""
    print(f"\n{'='*80}")
    print(f"üîç ENDPOINT: {endpoint_name}")
    print(f"{'='*80}")
    print(f"URL: {url}\n")
    
    print("üì• Fetching live data...")
    data, response_time = fetch_with_timing(url)
    
    if data is None:
        print(f"‚ùå Failed to fetch data")
        return
    
    print(f"‚úì Success! (Response time: {response_time:.3f}s)")
    
    # Analyze the data structure
    print(f"\nüìä Data Analysis:")
    print(f"  ‚Ä¢ Data type: {type(data).__name__}")
    print(f"  ‚Ä¢ Top-level keys: {list(data.keys()) if isinstance(data, dict) else 'N/A'}")
    
    if isinstance(data, dict):
        print(f"  ‚Ä¢ Total keys: {len(data)}")
        
        # Check for common timestamp fields
        if "last_updated" in data:
            print(f"  ‚Ä¢ Last updated: {data['last_updated']}")
        
        # Analyze nested structures
        for key, value in data.items():
            if isinstance(value, dict):
                print(f"    ‚îî‚îÄ {key}: {type(value).__name__} with {len(value)} items")
            elif isinstance(value, list):
                print(f"    ‚îî‚îÄ {key}: {type(value).__name__} with {len(value)} items")
            elif isinstance(value, str) and len(value) > 50:
                print(f"    ‚îî‚îÄ {key}: {type(value).__name__} ({len(value)} chars)")
            else:
                print(f"    ‚îî‚îÄ {key}: {value}")
    
    # Data size analysis
    json_str = json.dumps(data)
    json_size_kb = len(json_str.encode('utf-8')) / 1024
    print(f"\nüì¶ Data Size: {json_size_kb:.2f} KB")
    
    return data


def main():
    """Main testing function."""
    print("\n" + "‚ñà"*80)
    print("‚ñà" + " "*78 + "‚ñà")
    print("‚ñà" + " "*15 + "LIVE DATA ENDPOINT COMPARISON ANALYSIS" + " "*27 + "‚ñà")
    print("‚ñà" + " "*78 + "‚ñà")
    print("‚ñà"*80)
    
    try:
        # Load endpoints from metadata
        print("\nüîÑ Loading endpoint URLs from metadata...")
        endpoints = load_endpoints_from_metadata()
        print(f"‚úì Found {len(endpoints)} endpoints\n")
        
        # Display available endpoints
        print("Available endpoints:")
        for i, (name, url) in enumerate(endpoints.items(), 1):
            print(f"  {i}. {name}")
            print(f"     ‚îî‚îÄ {url}")
        
        # Test each endpoint
        print(f"\n{'='*80}")
        print("üöÄ FETCHING AND ANALYZING LIVE DATA")
        print(f"{'='*80}")
        
        collected_data = {}
        for endpoint_name, url in endpoints.items():
            try:
                data = compare_endpoint_data(endpoint_name, url)
                if data:
                    collected_data[endpoint_name] = data
            except Exception as e:
                print(f"\n‚ö†Ô∏è  Error processing {endpoint_name}: {str(e)}")
        
        # Summary
        print(f"\n{'='*80}")
        print("üìã SUMMARY")
        print(f"{'='*80}")
        print(f"\nSuccessfully fetched data from {len(collected_data)}/{len(endpoints)} endpoints\n")
        
        for endpoint_name in collected_data.keys():
            print(f"  ‚úì {endpoint_name}")
        
        # Insights
        print(f"\n{'='*80}")
        print("üí° KEY INSIGHTS")
        print(f"{'='*80}")
        
        insights = """
1. DATA SOURCES ARE IDENTICAL:
   ‚úì Both bike-share-json and gbfs-specification point to THE SAME live endpoint
   ‚úì The URLs are identical (same source)
   ‚úì The data retrieved will be the same

2. WHY TWO SEPARATE METADATA FILES?
   ‚Ä¢ They appear to be from a testing/validation process
   ‚Ä¢ Both are snapshots taken at nearly the same time (0.15 seconds apart)
   ‚Ä¢ Could be used to:
     - Test API consistency
     - Validate data schema compliance
     - Monitor extraction process reliability

3. DATA CHARACTERISTICS:
   ‚Ä¢ station_information: Contains list of bike station locations
   ‚Ä¢ station_status: Real-time availability of bikes/docks at each station
   ‚Ä¢ system_information: Overall system metadata
   ‚Ä¢ system_regions: Geographic regions served
   ‚Ä¢ system_pricing_plans: Pricing information

4. NEXT STEPS FOR YOUR PROJECT:
   ‚Ä¢ Only need to process ONE data source (they're identical)
   ‚Ä¢ Save processing by loading just bike-share-json
   ‚Ä¢ Both metadata files can be used for validation/monitoring
   ‚Ä¢ Actual live data comes from the URLs within these metadata files

5. ARCHITECTURE RECOMMENDATION:
   ‚îå‚îÄ Extract Metadata (feeds_summary.json)
   ‚îÇ  ‚îî‚îÄ Contains URLs to actual data endpoints
   ‚îÇ
   ‚îú‚îÄ Fetch from Live Endpoints (e.g., station_information)
   ‚îÇ  ‚îî‚îÄ This is where the REAL bike share data is
   ‚îÇ
   ‚îî‚îÄ Process in Layers:
      ‚îú‚îÄ BRONZE: Store raw JSON snapshots
      ‚îú‚îÄ SILVER: Transform and standardize
      ‚îî‚îÄ GOLD: Create analytics-ready tables
"""
        
        print(insights)
        
        print("\n" + "‚ñà"*80)
        print("‚ñà" + " "*20 + "ANALYSIS COMPLETE" + " "*42 + "‚ñà")
        print("‚ñà"*80 + "\n")
        
    except Exception as e:
        print(f"\n‚ùå Error during analysis: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
